{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# XLS-R (Wav2Vec2) CTC Fine-tuning on BAAI/CS-Dialogue (Mandarin\u2013English Code-Switching)\n", "\n", "**Goal:** Fine-tune `facebook/wav2vec2-xls-r-300m` with a custom CTC vocabulary built from CS-Dialogue.\n", "\n", "**Key points**\n", "- Uses `datasets.Audio` + `cast_column(..., Audio(16000))`.\n", "- Builds mixed EN (A\u2013Z) + Chinese vocab; maps space\u2192`|`.\n", "- Correct padding & metrics (WER/CER).\n", "\n", "References:\n", "- CS-Dialogue dataset card (16kHz, structure): https://huggingface.co/datasets/BAAI/CS-Dialogue\n", "- XLS-R-300M model card (16kHz input): https://huggingface.co/facebook/wav2vec2-xls-r-300m\n", "- Datasets audio processing: https://huggingface.co/docs/datasets/en/audio_process\n", "- WER / CER metrics: https://huggingface.co/spaces/evaluate-metric/wer , https://huggingface.co/spaces/evaluate-metric/cer\n"]}, {"cell_type": "code", "metadata": {"id": "install"}, "execution_count": null, "outputs": [], "source": ["%%bash\n", "pip -q install \"transformers==4.57.1\" \"datasets[audio]==2.21.0\" \"evaluate==0.4.2\" \"jiwer==3.0.4\" \\\n", "                 \"huggingface_hub>=0.24.0\" soundfile torchaudio --upgrade\n", "python - << 'PY'\n", "import transformers, datasets, evaluate\n", "print('Transformers:', transformers.__version__)\n", "print('Datasets    :', datasets.__version__)\n", "print('Evaluate    :', evaluate.__version__)\n", "PY\n"]}, {"cell_type": "code", "metadata": {"id": "imports"}, "execution_count": null, "outputs": [], "source": ["import os, re, tarfile, json\n", "from pathlib import Path\n", "from collections import Counter\n", "from typing import Dict, List\n", "import numpy as np, torch\n", "from datasets import Dataset, DatasetDict, Audio, Features, Value\n", "from huggingface_hub import hf_hub_download\n", "from transformers import AutoFeatureExtractor, AutoModelForCTC, TrainingArguments, Trainer, Wav2Vec2Processor, Wav2Vec2CTCTokenizer\n", "import evaluate\n", "\n", "SEED=42\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n", "\n", "BASE_DIR = Path('/content') if Path('/content').exists() else Path.cwd()\n", "DATA_ROOT = BASE_DIR / 'cs_dialogue'\n", "AUDIO_DIR = DATA_ROOT / 'short_wav'\n", "INDEX_DIR = DATA_ROOT / 'index' / 'short_wav'\n", "VOCAB_DIR = DATA_ROOT / 'custom_vocab'\n", "for p in [DATA_ROOT, AUDIO_DIR, INDEX_DIR, VOCAB_DIR]: p.mkdir(parents=True, exist_ok=True)\n", "\n", "CKPT='facebook/wav2vec2-xls-r-300m'\n", "NUM_SHARDS = int(os.environ.get('CS_NUM_SHARDS', 2))  # increase to 19 for full short_wav\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Download index & audio shards (short_wav)"]}, {"cell_type": "code", "metadata": {"id": "download"}, "execution_count": null, "outputs": [], "source": ["REPO_ID='BAAI/CS-Dialogue'\n", "def download_index():\n", "    pairs = [('train','data/index/short_wav/train/text'),\n", "             ('train','data/index/short_wav/train/wav.scp'),\n", "             ('dev','data/index/short_wav/dev/text'),\n", "             ('dev','data/index/short_wav/dev/wav.scp'),\n", "             ('test','data/index/short_wav/test/text'),\n", "             ('test','data/index/short_wav/test/wav.scp')]\n", "    for split, rel in pairs:\n", "        p = hf_hub_download(REPO_ID, rel)\n", "        dst = INDEX_DIR/ split / Path(rel).name\n", "        dst.parent.mkdir(parents=True, exist_ok=True)\n", "        os.replace(p, dst)\n", "    print('Index ready at', INDEX_DIR)\n", "\n", "def download_shards(n=2):\n", "    paths=[]\n", "    for i in range(n):\n", "        rel=f'data/short_wav/{i:02d}.tar.gz'\n", "        try:\n", "            p=hf_hub_download(REPO_ID, rel)\n", "            dst=DATA_ROOT/Path(rel).name\n", "            os.replace(p, dst)\n", "            paths.append(dst)\n", "            print('Downloaded', dst)\n", "        except Exception as e:\n", "            print('Skip', rel, e)\n", "    return paths\n", "\n", "def extract_all(tar_paths: List[Path], out_dir: Path):\n", "    out_dir.mkdir(parents=True, exist_ok=True)\n", "    for tp in tar_paths:\n", "        with tarfile.open(tp, 'r:gz') as tf:\n", "            tf.extractall(out_dir)\n", "        print('Extracted', tp, '->', out_dir)\n", "\n", "download_index()\n", "tars=download_shards(NUM_SHARDS)\n", "extract_all(tars, AUDIO_DIR)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Build DatasetDict from wav.scp & text"]}, {"cell_type": "code", "metadata": {"id": "dataset"}, "execution_count": null, "outputs": [], "source": ["def read_kv(fp: Path):\n", "    d={}\n", "    with open(fp, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            line=line.strip()\n", "            if not line: continue\n", "            k,v=line.split(' ',1)\n", "            d[k]=v\n", "    return d\n", "\n", "def make_split(split: str):\n", "    wavscp = read_kv(INDEX_DIR/split/'wav.scp')\n", "    text   = read_kv(INDEX_DIR/split/'text')\n", "    ids, paths, trans = [], [], []\n", "    for uid, wavpath in wavscp.items():\n", "        # expect .../short_wav/{shard}/{file}.wav\n", "        shard = Path(wavpath).parts[-2]\n", "        fname = Path(wavpath).name\n", "        local = AUDIO_DIR / shard / fname\n", "        if local.exists() and uid in text:\n", "            ids.append(uid); paths.append(str(local)); trans.append(text[uid])\n", "    feats = Features({'id': Value('string'), 'audio': Audio(sampling_rate=16000), 'transcription': Value('string')})\n", "    return Dataset.from_dict({'id': ids, 'audio': paths, 'transcription': trans}, features=feats)\n", "\n", "train_ds = make_split('train')\n", "val_ds   = make_split('dev')\n", "test_ds  = make_split('test')\n", "minds = DatasetDict(train=train_ds, validation=val_ds, test=test_ds)\n", "minds = minds.cast_column('audio', Audio(sampling_rate=16000))\n", "minds\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Normalize transcripts (EN upper + Chinese)"]}, {"cell_type": "code", "metadata": {"id": "normalize"}, "execution_count": null, "outputs": [], "source": ["CN_RANGE = r\"\\u4E00-\\u9FFF\"\n", "def normalize_text(ex):\n", "    t = ex['transcription'].strip().upper()\n", "    t = re.sub(fr\"[^{CN_RANGE}A-Z' ]+\", \"\", t)\n", "    t = re.sub(r\"\\s+\", \" \", t)\n", "    return {'transcription': t}\n", "\n", "minds = minds.map(normalize_text)\n", "minds['train'][0]['transcription'][:120]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Build CTC vocab (space\u2192`|`)"]}, {"cell_type": "code", "metadata": {"id": "vocab"}, "execution_count": null, "outputs": [], "source": ["from collections import Counter\n", "def collect_chars(ds, key='transcription'):\n", "    c=Counter()\n", "    for s in ds[key]: c.update(list(s))\n", "    return c\n", "cnt=Counter()\n", "for sp in ['train','validation','test']:\n", "    cnt.update(collect_chars(minds[sp]))\n", "chars=sorted([ch for ch in cnt if ch!=' '])\n", "vocab={ch:i for i,ch in enumerate(chars)}\n", "vocab['|']=len(vocab); vocab['<unk>']=len(vocab); vocab['<pad>']=len(vocab)\n", "VOCAB_DIR.mkdir(parents=True, exist_ok=True)\n", "with open(VOCAB_DIR/'vocab.json','w',encoding='utf-8') as f:\n", "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n", "len(vocab)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Init tokenizer/processor & XLS-R-300M model"]}, {"cell_type": "code", "metadata": {"id": "init"}, "execution_count": null, "outputs": [], "source": ["feature_extractor = AutoFeatureExtractor.from_pretrained(CKPT, return_attention_mask=True)\n", "tokenizer = Wav2Vec2CTCTokenizer(str(VOCAB_DIR/'vocab.json'), unk_token='<unk>', pad_token='<pad>', word_delimiter_token='|')\n", "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n", "\n", "model = AutoModelForCTC.from_pretrained(\n", "    CKPT,\n", "    ctc_loss_reduction='mean',\n", "    pad_token_id=processor.tokenizer.pad_token_id,\n", "    vocab_size=len(processor.tokenizer)\n", ")\n", "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "model.to(device); model.train(); device\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Encode \u2192 input_values / attention_mask / labels"]}, {"cell_type": "code", "metadata": {"id": "encode"}, "execution_count": null, "outputs": [], "source": ["def prepare_batch(batch):\n", "    audio = batch['audio']\n", "    ins = processor(audio['array'], sampling_rate=audio['sampling_rate'], return_attention_mask=True)\n", "    batch['input_values']   = ins['input_values'][0]\n", "    batch['attention_mask'] = ins['attention_mask'][0]\n", "    with processor.as_target_processor():\n", "        batch['labels'] = processor(batch['transcription']).input_ids\n", "    return batch\n", "encoded = minds.map(prepare_batch, remove_columns=minds['train'].column_names, num_proc=1)\n", "encoded\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Collator & Metrics (WER/CER) + Sanity check"]}, {"cell_type": "code", "metadata": {"id": "collate_metrics"}, "execution_count": null, "outputs": [], "source": ["from dataclasses import dataclass\n", "from typing import Union\n", "@dataclass\n", "class DataCollatorCTCWithPadding:\n", "    processor: Wav2Vec2Processor\n", "    padding: Union[bool,str]='longest'\n", "    def __call__(self, features: List[Dict]):\n", "        inf = [{'input_values': f['input_values']} for f in features]\n", "        lab = [{'input_ids': f['labels']} for f in features]\n", "        batch = self.processor.pad(inf, padding=self.padding, return_tensors='pt')\n", "        with self.processor.as_target_processor():\n", "            labels_batch = self.processor.pad(lab, padding=self.padding, return_tensors='pt')\n", "        labels = labels_batch['input_ids'].masked_fill(labels_batch['attention_mask'].ne(1), -100)\n", "        batch['labels']=labels\n", "        return batch\n", "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n", "\n", "wer_metric = evaluate.load('wer')\n", "cer_metric = evaluate.load('cer')\n", "def compute_metrics(pred):\n", "    pred_ids = np.argmax(pred.predictions, axis=-1)\n", "    label_ids = pred.label_ids.copy()\n", "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n", "    pred_str  = processor.batch_decode(pred_ids,  skip_special_tokens=True)\n", "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n", "    return {'wer': wer_metric.compute(predictions=pred_str, references=label_str),\n", "            'cer': cer_metric.compute(predictions=pred_str, references=label_str)}\n", "\n", "# sanity check\n", "sample = encoded['validation'].select(range(min(3, len(encoded['validation']))))\n", "if len(sample):\n", "    ins = processor.pad({'input_values': sample['input_values']}, padding=True, return_tensors='pt')\n", "    with torch.no_grad():\n", "        lg = model(input_values=ins['input_values'].to(device), attention_mask=ins['attention_mask'].to(device)).logits\n", "    hyp_ids = lg.argmax(dim=-1).cpu().numpy()\n", "    lbl_ids=[]\n", "    for seq in sample['labels']:\n", "        arr=np.array(seq, dtype=np.int64); arr[arr==-100]=processor.tokenizer.pad_token_id; lbl_ids.append(arr.tolist())\n", "    hyp = processor.batch_decode(hyp_ids, skip_special_tokens=True)\n", "    ref = processor.batch_decode(lbl_ids, skip_special_tokens=True)\n", "    for i,(r,h) in enumerate(zip(ref,hyp),1):\n", "        print(f'[{i}] REF: {r[:80]}')\n", "        print(f'[{i}] HYP: {h[:80]}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Train"]}, {"cell_type": "code", "metadata": {"id": "train"}, "execution_count": null, "outputs": [], "source": ["from transformers import TrainingArguments, Trainer\n", "args = TrainingArguments(\n", "    output_dir=str(DATA_ROOT/'outputs'),\n", "    per_device_train_batch_size=8,\n", "    per_device_eval_batch_size=8,\n", "    gradient_accumulation_steps=2,\n", "    learning_rate=1e-4,\n", "    warmup_steps=500,\n", "    max_steps=2000,  # increase to 8000-10000 later\n", "    gradient_checkpointing=True,\n", "    fp16=torch.cuda.is_available(),\n", "    group_by_length=True,\n", "    evaluation_strategy='steps',\n", "    eval_steps=200,\n", "    save_steps=1000,\n", "    logging_steps=25,\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model='cer',\n", "    greater_is_better=False,\n", "    report_to='none',\n", ")\n", "trainer = Trainer(\n", "    model=model,\n", "    args=args,\n", "    train_dataset=encoded['train'],\n", "    eval_dataset=encoded['validation'],\n", "    tokenizer=processor,\n", "    data_collator=data_collator,\n", "    compute_metrics=compute_metrics,\n", ")\n", "trainer.train()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}